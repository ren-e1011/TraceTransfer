{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylangacq as pla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pla.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import string\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.max_seq_items = 2000\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore header for easier (ordinal) columnar indexing\n",
    "datasheet = pd.read_excel('CogData_FU_82818.xlsx',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                             Record ID\n",
       "1                                            Event Name\n",
       "2     Write a short sketch about a memory from your ...\n",
       "3     How does technology and social media impact th...\n",
       "4                   Date of Neurobehavioral Status Exam\n",
       "                            ...                        \n",
       "92                         Cog State: One Back Accuracy\n",
       "93                          Ravens Progressive Matrices\n",
       "94                                Logical Memory Part B\n",
       "95                         East Boston Immediate Recall\n",
       "96                                            Complete?\n",
       "Name: 0, Length: 97, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column headers\n",
    "# 2 - memory responses\n",
    "# 3 - tech responses\n",
    "datasheet.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row Indices of Memory Responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore first (header) column\n",
    "memory_ix = datasheet.iloc[1:][datasheet.iloc[1:,2].notnull()].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record IDs of Memory Respondents [Analyze all scores from each participant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_id = list(datasheet.iloc[memory_ix][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tech respondents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_ix = datasheet.iloc[1:][datasheet.iloc[1:,3].notnull()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_id = list(datasheet.iloc[tech_ix][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#each respondent responded to one prompt\n",
    "len(tech_id) == len(set(tech_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total MMSE score'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 74th col lists the participant's mmse \n",
    "datasheet.iloc[0][74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_score_latest_completed = []\n",
    "for ix in tech_id:\n",
    "    slice_ix = list(datasheet[datasheet[0]==ix].index)\n",
    "#     while datasheet.iloc[max(slice_ix)][96] != 'Complete':\n",
    "    # if there is no mmse score, take most recent score\n",
    "    while np.isnan(datasheet.iloc[max(slice_ix)][74]):\n",
    "        slice_ix.remove(max(slice_ix))\n",
    "    ix_score_latest_completed += [max(slice_ix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_ix_score_latest_completed = [i for i in ix_score_latest_completed if datasheet.iloc[i,0] in memory_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_mmse = pd.Series([y for y in datasheet.iloc[ix_score_latest_completed,74]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ix by samples \n",
    "tech_mmse_dict = {ix:score for (ix,score) in zip(tech_ix,tech_mmse)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(tech_mmse_dict,'tech_mmse_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_mmse = pd.Series([y for y in datasheet.iloc[mem_ix_score_latest_completed,74]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_mmse_dict = {ix:score for (ix,score) in zip(memory_ix,mem_mmse)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(mem_mmse_dict,'mem_mmse_dict.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load addb vocab glove word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = './PretrainedWordEmb/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions - tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns nested list\n",
    "\n",
    "def list_tokenize(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    return [nltk.word_tokenize(sent) for sent in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I',\n",
       "  'have',\n",
       "  'great',\n",
       "  'memories',\n",
       "  'of',\n",
       "  'growing',\n",
       "  'up',\n",
       "  'in',\n",
       "  'huge',\n",
       "  'house',\n",
       "  'with',\n",
       "  'a',\n",
       "  'large',\n",
       "  'garden',\n",
       "  'full',\n",
       "  'of',\n",
       "  'flowers',\n",
       "  'and',\n",
       "  'fruit',\n",
       "  'trees',\n",
       "  '.'],\n",
       " ['I',\n",
       "  'specifically',\n",
       "  'remember',\n",
       "  'enjoying',\n",
       "  'climbing',\n",
       "  'the',\n",
       "  'trees',\n",
       "  'to',\n",
       "  'pick',\n",
       "  'plums',\n",
       "  ',',\n",
       "  'pears',\n",
       "  'and',\n",
       "  'peaches',\n",
       "  'and',\n",
       "  'maybe',\n",
       "  'even',\n",
       "  'apples',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'lower',\n",
       "  'portion',\n",
       "  'of',\n",
       "  'the',\n",
       "  'garden',\n",
       "  'was',\n",
       "  'reserved',\n",
       "  'for',\n",
       "  'vegetables',\n",
       "  'and',\n",
       "  'I',\n",
       "  'was',\n",
       "  \"n't\",\n",
       "  'too',\n",
       "  'interested',\n",
       "  'in',\n",
       "  'that',\n",
       "  '.'],\n",
       " ['This',\n",
       "  'is',\n",
       "  'one',\n",
       "  'of',\n",
       "  'my',\n",
       "  'best',\n",
       "  'childhood',\n",
       "  'memories',\n",
       "  'of',\n",
       "  'being',\n",
       "  'outdoors',\n",
       "  '.']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tokenize(datasheet.iloc[memory_ix[6],2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.nltk.org/book/ch07.html\n",
    "def get_pos(document):\n",
    "    #sentence segmentation\n",
    "    sentences = nltk.sent_tokenize(document) \n",
    "    #tokenization\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    #pos tagging\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences] \n",
    "    \n",
    "    pos_list = [p for sent in sentences for (t,p) in sent]\n",
    "    return pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRP',\n",
       " 'VBP',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'VBG',\n",
       " 'RP',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'JJ',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'CC',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " '.',\n",
       " 'PRP',\n",
       " 'RB',\n",
       " 'VBP',\n",
       " 'VBG',\n",
       " 'VBG',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'NNS',\n",
       " 'CC',\n",
       " 'NNS',\n",
       " 'CC',\n",
       " 'RB',\n",
       " 'RB',\n",
       " 'NNS',\n",
       " '.',\n",
       " 'DT',\n",
       " 'JJR',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'CC',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'RB',\n",
       " 'RB',\n",
       " 'JJ',\n",
       " 'IN',\n",
       " 'DT',\n",
       " '.',\n",
       " 'DT',\n",
       " 'VBZ',\n",
       " 'CD',\n",
       " 'IN',\n",
       " 'PRP$',\n",
       " 'JJS',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'VBG',\n",
       " 'NNS',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pos(datasheet.iloc[memory_ix[6],2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab word embeddings for APC Vocabulary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabset = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in tech_ix:\n",
    "    for sent in list_tokenize(datasheet.iloc[ix,3]):\n",
    "        vocabset.update(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in memory_ix:\n",
    "    for sent in list_tokenize(datasheet.iloc[ix,2]):\n",
    "        vocabset.update(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2125"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map APC POS (nltk): ADDB POS (transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "posset = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/renee/Documents/CT_Fa18/Spec/Trace'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "addb_posset = torch.load('pos_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in memory_ix:\n",
    "    posset.update(get_pos(datasheet.iloc[ix,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in tech_ix:\n",
    "    posset.update(get_pos(datasheet.iloc[ix,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$',\n",
       " \"''\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " ':',\n",
       " 'CC',\n",
       " 'CD',\n",
       " 'DT',\n",
       " 'EX',\n",
       " 'FW',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'JJR',\n",
       " 'JJS',\n",
       " 'MD',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNPS',\n",
       " 'NNS',\n",
       " 'PDT',\n",
       " 'POS',\n",
       " 'PRP',\n",
       " 'PRP$',\n",
       " 'RB',\n",
       " 'RBR',\n",
       " 'RBS',\n",
       " 'RP',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'VBD',\n",
       " 'VBG',\n",
       " 'VBN',\n",
       " 'VBP',\n",
       " 'VBZ',\n",
       " 'WDT',\n",
       " 'WP',\n",
       " 'WRB'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posset.difference(addb_posset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swap with chat term where poss\n",
    "posdict = {k:k for k in posset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References for mapping\n",
    "# https://talkbank.org/manuals/MOR.html#_Toc17967934\n",
    "# https://talkbank.org/manuals/MOR.html#_Toc1313898\n",
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "posdict['CC'] = 'CONJ'\n",
    "\n",
    "posdict['CD'] = 'DET:NUM'\n",
    "\n",
    "posdict['DT'] = 'DET:ART'\n",
    "\n",
    "posdict['EX'] = 'PRO:EXIST'\n",
    "\n",
    "posdict['IN'] = 'PREP'\n",
    "\n",
    "posdict['JJ'] = 'ADJ'\n",
    "\n",
    "posdict['JJR'] = 'ADJ'\n",
    "\n",
    "posdict['JJS'] = 'ADJ'\n",
    "\n",
    "posdict['MD'] = 'MOD'\n",
    "\n",
    "posdict['NN'] = 'N'\n",
    "\n",
    "posdict['NNP'] = 'N:PROP'\n",
    "\n",
    "posdict['NNPS'] = 'N:PROP'\n",
    "\n",
    "posdict['NNS'] = 'N'\n",
    "\n",
    "posdict['PDT'] = 'DET:DEM'\n",
    "\n",
    "posdict['POS'] = 'DET:POSS'\n",
    "\n",
    "posdict['PRP'] = 'PRO:PER'\n",
    "\n",
    "posdict['PRP$'] = 'PRO:POSS'\n",
    "\n",
    "posdict['RB'] = 'ADV'\n",
    "\n",
    "posdict['RBR'] = 'ADV'\n",
    "\n",
    "posdict['RBS'] = 'ADV'\n",
    "\n",
    "posdict['RP'] = 'PART'\n",
    "\n",
    "posdict['TO'] = 'PREP'\n",
    "\n",
    "posdict['VB'] = 'V'\n",
    "\n",
    "posdict['VBD'] = 'V'\n",
    "\n",
    "posdict['VBG'] = 'PART'\n",
    "\n",
    "posdict['VBN']  = 'PART'\n",
    "\n",
    "posdict['VBP'] = 'V'\n",
    "\n",
    "posdict['VBZ'] = 'V'\n",
    "\n",
    "# DET:INT not in addb_posseet\n",
    "posdict['WDT'] = 'DET:POSS'\n",
    "\n",
    "posdict['WP'] = 'PRO:INT'\n",
    "\n",
    "# ADV:WH not in addb_posset\n",
    "posdict['WRB'] = 'ADV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[')', '(', ',', '$', ':', 'FW', \"''\"]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[missing for missing in posset.difference(set(addb_posset.keys())) if posdict[missing] not in set(addb_posset.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(posdict,'apc_pos_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posdict = torch.load('apc_pos_dict.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VOCAB EMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Blosc/bcolz\n",
    "import bcolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/renee/Documents/CT_Fa18/Spec/Trace'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with zipfile.ZipFile('./PretrainedWordEmb/glove.42B.300d.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./PretrainedWordEmb/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load pretrained glove (common crawl 42B tokens, 1.9M vocab, uncased, 300d vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = './PretrainedWordEmb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'{glove_path}/glove.42B.300d.txt', 'rb') as f:\n",
    "#     for l in f:\n",
    "#         line = l.decode().split()\n",
    "#         word = line[0]\n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = []\n",
    "# idx = 0\n",
    "# word2idx = {}\n",
    "# vectors = bcolz.carray(np.zeros(1), rootdir=f'{glove_path}/glove.42B.300.dat', mode='w')\n",
    "\n",
    "# with open(f'{glove_path}/glove.42B.300d.txt', 'rb') as f:\n",
    "#     for l in f:\n",
    "#         line = l.decode().split()\n",
    "#         word = line[0]\n",
    "#         words.append(word)\n",
    "#         word2idx[word] = idx\n",
    "#         idx += 1\n",
    "#         vect = np.array(line[1:]).astype(np.float)\n",
    "#         vectors.append(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = bcolz.carray(vectors[1:].reshape((-1,300)), rootdir=f'{glove_path}/glove.42B.300.dat', mode='w')\n",
    "# vectors.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(words, open(f'{glove_path}/glove.42B.300_words.pkl', 'wb'))\n",
    "# pickle.dump(word2idx, open(f'{glove_path}/glove.42B.300_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = bcolz.open(f'{glove_path}/glove.42B.300.dat')[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pickle.load(open(f'{glove_path}/glove.42B.300_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'{glove_path}/glove.42B.300_idx.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNK token -- average of embeddings\n",
    "average_vec = np.mean(vectors, axis=0)\n",
    "# print(average_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'UNK' in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace unknown word with average embedding\n",
    "words.append('UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1917494, 300)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.concatenate((vectors,np.reshape(average_vec,(1,300))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1917495"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx['UNK'] = len(vectors) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1917495"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "def get_glovembs(sample):\n",
    "    \n",
    "    def handle_contractions(x):\n",
    "        tokenizer = TreebankWordTokenizer()\n",
    "        x = tokenizer.tokenize(x)\n",
    "        x = ' '.join(x)\n",
    "        return x\n",
    "    \n",
    "    # preprocess acc to pretrained embeddings \n",
    "    # lower\n",
    "    # replace _, -\n",
    "    # rm contractions\n",
    "    # rm nonascii chars\n",
    "    \n",
    "    sample = [s for s in sample]\n",
    "    \n",
    "    preprocessample = [handle_contractions(t).split()[0] for t in [token.lower().replace('_','-') for token in sample]]\n",
    "    \n",
    "    \n",
    "    preprocessample = [''.join([i if ord(i) < 128 else '' for i in w]) for w in preprocessample]\n",
    "    \n",
    "    preprocessample = [t.translate(str.maketrans('', '', string.punctuation)) for t in preprocessample]\n",
    "    \n",
    "# uncomment for traceunks = get_glovembs(vocabset)\n",
    "#     return set([word for word in preprocessample if word not in words])\n",
    "    # {w: vectors[word2idx[w]] for w in words}\n",
    "    return {sample[i]: torch.tensor(vectors[word2idx[preprocessample[i]]]) if preprocessample[i] in words else torch.tensor(vectors[word2idx['UNK']]) for i in range(len(sample))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "traceunks = get_glovembs(vocabset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "traceunks = set()\n",
    "for word in vocabset:\n",
    "    if word not in words:\n",
    "        traceunks.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " '11alpha',\n",
       " '2ndteam',\n",
       " 'devicelogs',\n",
       " 'dontuse',\n",
       " 'electroniceven',\n",
       " 'envirmental',\n",
       " 'etccwas',\n",
       " 'gerslyn',\n",
       " 'intigral',\n",
       " 'monmarte',\n",
       " 'precomputer',\n",
       " 'preinternet',\n",
       " 'townno',\n",
       " 'unqua',\n",
       " 'windowdoors',\n",
       " 'wlodawski'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traceunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traceunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_vocab_embs = get_glovembs(vocabset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2125"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trace_vocab_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(trace_vocab_embs, open(f'{glove_path}/trace.vocab.glove.42B.300_words.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python dl1010",
   "language": "python",
   "name": "dl1010"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
