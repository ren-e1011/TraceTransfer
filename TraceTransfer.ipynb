{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylangacq as pla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/renee/Documents/CT_Fa18/Spec/Trace'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mem = torch.load('mem_mmse_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tech = torch.load('tech_mmse_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_datasheet = pd.read_excel('CogData_FU_82818.xlsx',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                             Record ID\n",
       "1                                            Event Name\n",
       "2     Write a short sketch about a memory from your ...\n",
       "3     How does technology and social media impact th...\n",
       "4                   Date of Neurobehavioral Status Exam\n",
       "                            ...                        \n",
       "92                         Cog State: One Back Accuracy\n",
       "93                          Ravens Progressive Matrices\n",
       "94                                Logical Memory Part B\n",
       "95                         East Boston Immediate Recall\n",
       "96                                            Complete?\n",
       "Name: 0, Length: 97, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column headers: 2-memory, 3-tech\n",
    "apc_datasheet.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6      When I was a little child I had the same exper...\n",
       "29     When I was very young, living in the Astoria n...\n",
       "51     I went to Paris when I was 16.  I was staying ...\n",
       "56     My husband and I had always wanted to go to Pa...\n",
       "78     I was born in Hungary. A significant memory is...\n",
       "89     When I was 16 years old I traveled to Europe b...\n",
       "100    I have great memories of growing up in huge ho...\n",
       "107    When I was 8 or 9 years old, I was playing on ...\n",
       "118    One of my fondest childhood memories is how mu...\n",
       "154    One of my favorite memories from my childhood ...\n",
       "184    My favorite pastime as a small child was re-en...\n",
       "202    I remember going to the park with my dad to pl...\n",
       "217    At the age of 14, I was the setter on a volley...\n",
       "226    I have been able to recall - in detail -- memo...\n",
       "242    I have a very clear memeory of last time I saw...\n",
       "266    My parents lived in Ft. Myers, Florida, when I...\n",
       "277    i visited Washington DC with my brother, my gr...\n",
       "286    I remember finally going to the country in the...\n",
       "297    My dad took my friends and cousins  and me cam...\n",
       "303    A vivid memory of my childhood was about my fa...\n",
       "324    My dad was off on Wednesday's and we often wen...\n",
       "347         My earliest memory is standing, in diaper...\n",
       "352    We took very few trips as a family. We had lim...\n",
       "359    They say traumatic memories tend to remain viv...\n",
       "375    I remember that in fourth grade, there was an ...\n",
       "387    My mother frequently threatened to kill hersel...\n",
       "398    I had auditioned for a repertory company in my...\n",
       "411    learning to ride a bike at the jersey shore on...\n",
       "423    In fifth grade my grandparents took my sister ...\n",
       "433    I have fond memories of going to TOBAY beach o...\n",
       "438    I was 13, & 8 of our 9 family members set off ...\n",
       "445    When I was seven my father went off to play te...\n",
       "448    Two memories come to mind in response to this ...\n",
       "460    I remember how we celebrated the Jewish holida...\n",
       "464    I spent a lot of time as an only child growing...\n",
       "470    I remember riding on my father's shoulders.  I...\n",
       "474    My family went on a vacation to Hawaii, Oahu. ...\n",
       "477    When I was young we used to take one family va...\n",
       "496             This is too much pressure for a writer. \n",
       "499    My mother's best friend drove a convertible.  ...\n",
       "502       When I visited relatives in Alabama, I enjo...\n",
       "506    When I was in 8th grade I flew to Sea Island G...\n",
       "509    I loved growing up in Israel and being in a sa...\n",
       "522    This is my kids favorite story about what it w...\n",
       "537    When I was young I went to a sleep away camp e...\n",
       "540    My father was in charge of building the backpa...\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apc_datasheet.iloc[list(y_mem.keys()),2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6      I stay in touch with family and friends throug...\n",
       "29     I'm a journalist who has covered the rise of t...\n",
       "51     My husband and I are on our computers all day ...\n",
       "56     I'm on the computer all the time, writing or r...\n",
       "78     My use of social media is minimal. I only use ...\n",
       "89     I check that on what my children are doing and...\n",
       "100    Technology has helped me connect to family and...\n",
       "107    I use a laptop and phone constantly for work a...\n",
       "118    I do not use any social media sites at this ti...\n",
       "154    Social media is great to stay in touch with wh...\n",
       "184    Social Media is a bit of a challenge as I am a...\n",
       "202    I use email daily for work and it usually cont...\n",
       "217    Technology is woven into the fabric of our liv...\n",
       "226    We dont engage on social media. Email exchange...\n",
       "234    I'm writing correspondence via computer most o...\n",
       "242    If by Technology we refer  to communication te...\n",
       "266    Social media not much, since I don'tuse them, ...\n",
       "277    I work in a business that is driven by technol...\n",
       "280                   Does not affect my life or family.\n",
       "286    I use my smart phone often in my iPad and try ...\n",
       "297    It is ever present although i see it most prob...\n",
       "303    Since I worked up until 2013, I had adjusted t...\n",
       "324                       It's easy to be on it too much\n",
       "337    Technology has interrupted my personal life th...\n",
       "347         Technology and social media have had huge...\n",
       "352    Technology and the internet give us access to ...\n",
       "359    Technology and social media is a double edged ...\n",
       "375    Technology gives us access to an unbelievable ...\n",
       "387    It is both a help and a hindrance because it i...\n",
       "398    It keeps me in the know about people (friends ...\n",
       "411    Provides an efficient way to gain access to re...\n",
       "423    Don't care for social media. Although I have a...\n",
       "433    I don't use social media, as I hear that it co...\n",
       "438    Good: keep up with friends & family, exposure ...\n",
       "445    Technology and social media have become part o...\n",
       "448    At this time in my life, technology has just b...\n",
       "460    I find technology a wonderful way for me to st...\n",
       "464    I think social media is incredibly distracting...\n",
       "470    Two steps forward, one back.  Technology is am...\n",
       "474    I have a Facebook account, but I only subscrib...\n",
       "477    I spend all day at the beck and call of my sma...\n",
       "496    Okay... I'll combine a bit of question 1 with ...\n",
       "499    I use a computer for emails, work, research an...\n",
       "502    Technology is a time killer that speeds everyt...\n",
       "506    I use my computer or tablet and my phone daily...\n",
       "509    I try not to spend time on facebook--I don't f...\n",
       "519    I feel that a lot of times technology and soci...\n",
       "522    I'm glad I had my children fairly late, becaus...\n",
       "537    Technology is a very important part of my life...\n",
       "540    Technology impacts me and my family on a daily...\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apc_datasheet.iloc[list(y_tech.keys()),3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Vocab Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = './PretrainedWordEmb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_emb = pickle.load(open(f'{glove_path}/trace.vocab.glove.42B.300_words.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2125"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_addb_posdict = torch.load('apc_pos_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CD': 'DET:NUM',\n",
       " 'FW': 'FW',\n",
       " 'EX': 'PRO:EXIST',\n",
       " 'WP': 'PRO:INT',\n",
       " 'NNS': 'N',\n",
       " 'WRB': 'ADV',\n",
       " '(': '(',\n",
       " 'RB': 'ADV',\n",
       " 'PDT': 'DET:DEM',\n",
       " 'VBG': 'PART',\n",
       " 'CC': 'CONJ',\n",
       " \"''\": \"''\",\n",
       " ':': ':',\n",
       " 'VBP': 'V',\n",
       " 'IN': 'PREP',\n",
       " 'TO': 'PREP',\n",
       " 'MD': 'MOD',\n",
       " 'PRP$': 'PRO:POSS',\n",
       " ')': ')',\n",
       " 'PRP': 'PRO:PER',\n",
       " 'JJ': 'ADJ',\n",
       " '.': '.',\n",
       " 'JJR': 'ADJ',\n",
       " 'VBD': 'V',\n",
       " 'RBR': 'ADV',\n",
       " 'RP': 'PART',\n",
       " ',': ',',\n",
       " 'VB': 'V',\n",
       " 'VBN': 'PART',\n",
       " 'NN': 'N',\n",
       " 'NNPS': 'N:PROP',\n",
       " 'WDT': 'DET:POSS',\n",
       " 'POS': 'DET:POSS',\n",
       " '$': '$',\n",
       " 'NNP': 'N:PROP',\n",
       " 'JJS': 'ADJ',\n",
       " 'VBZ': 'V',\n",
       " 'RBS': 'ADV',\n",
       " 'DT': 'DET:ART'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trace nltk pos: addb transcribed pos\n",
    "trace_addb_posdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = torch.load('./pos_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'CM': 1,\n",
       " 'COMP': 2,\n",
       " 'PRO:EXIST': 3,\n",
       " 'IN#ADJ': 4,\n",
       " 'PRE#PART': 5,\n",
       " '.': 6,\n",
       " 'PRE#V': 7,\n",
       " 'OVER#PART': 8,\n",
       " 'UN#ADV': 9,\n",
       " 'PRO:PER': 10,\n",
       " 'ADV:TEM': 11,\n",
       " '+/.': 12,\n",
       " 'V': 13,\n",
       " 'PRO:INT': 14,\n",
       " 'N': 15,\n",
       " 'PRO:POSS': 16,\n",
       " 'N:ADJ': 17,\n",
       " 'DET:DEM': 18,\n",
       " 'END': 19,\n",
       " 'PRO:INDEF': 20,\n",
       " '+\"/.': 21,\n",
       " 'DET:POSS': 22,\n",
       " 'OVER#V': 23,\n",
       " 'DET:ART': 24,\n",
       " 'INF': 25,\n",
       " 'OVER#N:GERUND': 26,\n",
       " 'META': 27,\n",
       " 'ON': 28,\n",
       " 'UP#V': 29,\n",
       " 'MID#N': 30,\n",
       " 'NEG': 31,\n",
       " 'MOD': 32,\n",
       " '+...': 33,\n",
       " 'PRO:SUB': 34,\n",
       " 'UN#PART': 35,\n",
       " 'PREP': 36,\n",
       " 'DET:NUM': 37,\n",
       " 'N:GERUND': 38,\n",
       " '+//?': 39,\n",
       " 'UN#ADJ': 40,\n",
       " 'COP': 41,\n",
       " 'N:PT': 42,\n",
       " 'AUX': 43,\n",
       " 'ADV': 44,\n",
       " 'OUT#PART': 45,\n",
       " '!': 46,\n",
       " 'PRO:OBJ': 47,\n",
       " 'ADJ': 48,\n",
       " 'N:PROP': 49,\n",
       " 'MINI#N': 50,\n",
       " '+/?': 51,\n",
       " '?': 52,\n",
       " 'POST': 53,\n",
       " 'N:LET': 54,\n",
       " 'UN#N': 55,\n",
       " 'UP#PART': 56,\n",
       " 'COORD': 57,\n",
       " 'QN': 58,\n",
       " 'GRAND#N': 59,\n",
       " 'CONJ': 60,\n",
       " '+..?': 61,\n",
       " 'PART': 62,\n",
       " 'BEG': 63,\n",
       " 'PRO:REL': 64,\n",
       " 'PRO:REFL': 65,\n",
       " 'NEO': 66,\n",
       " 'CO': 67,\n",
       " '+\".': 68,\n",
       " 'PRO:DEM': 69,\n",
       " '+//.': 70}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group minibatches of similar size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embs(batchsize=1, mem_ix=[],tech_ix=[],pad=True):\n",
    "    \n",
    "    def list_tokenize(document):\n",
    "        sentences = nltk.sent_tokenize(document)\n",
    "        sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "        \n",
    "        return [token for sentence in sentences for token in sentence]\n",
    "    \n",
    "    def get_pos(document):\n",
    "        #sentence segmentation\n",
    "        sentences = nltk.sent_tokenize(document) \n",
    "        #tokenization\n",
    "        sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "        #pos tagging\n",
    "        sentences = [nltk.pos_tag(sent) for sent in sentences] \n",
    "\n",
    "        pos_list = [p for sent in sentences for (t,p) in sent]\n",
    "        return pos_list\n",
    "\n",
    "    def num_vectorize(targets):\n",
    "        vectors = []\n",
    "        for i in targets:\n",
    "            v = torch.zeros([30])\n",
    "            # up until MMSE index...greater than index i \n",
    "            v[:i] = 1\n",
    "            vectors.append(v)\n",
    "        return torch.stack(vectors).double()\n",
    "    \n",
    "    def pad_minibatch(minib):\n",
    "    \n",
    "        batchsize = len(minib)\n",
    "\n",
    "        seq_lens = [len(emb) for emb in minib]\n",
    "        max_len = max(seq_lens)\n",
    "\n",
    "        # input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "    #     seq_tensor = torch.zeros((batchsize, max_len, EMBEDDING_SIZE)).double()\n",
    "        seq_tensor = torch.zeros((max_len, batchsize, EMBEDDING_SIZE)).double()\n",
    "\n",
    "\n",
    "        for i, (seq,length) in enumerate( zip(minib,seq_lens) ):\n",
    "            for wi,word in enumerate(seq):\n",
    "    #             seq_tensor[i,wi] = word\n",
    "                seq_tensor[wi,i] = word\n",
    "        # mod to sort at the start?\n",
    "        seq_tensor = nn.utils.rnn.pack_padded_sequence(seq_tensor,lengths=seq_lens,enforce_sorted=False)\n",
    "        return seq_tensor\n",
    "\n",
    "    \n",
    "    embeddings = []\n",
    "    targets = []\n",
    "\n",
    "\n",
    "    for ix_list,data,targetdict in [(mem_ix,apc_datasheet[2],y_mem),(tech_ix,apc_datasheet[3],y_tech)]:\n",
    "        \n",
    "        for ix in ix_list:\n",
    "            embedding = [(vocab_emb[token],torch.zeros(len(pos_dict),dtype=torch.float64),trace_addb_posdict[pos]) for (token,pos) in zip(list_tokenize(data.iloc[ix]), get_pos(data.iloc[ix]))]\n",
    "            target = targetdict[ix]\n",
    "\n",
    "            emb = []\n",
    "            for tkn in embedding:\n",
    "                if tkn[2] in pos_dict:\n",
    "                    tkn[1][pos_dict[tkn[2]]] = 1\n",
    "                    # parts of speech [')', '(', ',', '$', ':', 'FW', \"''\"] not in addb dataset --rm corresponding tokens (transcripts on which model was trained dont have foreign words, complete puctuation set)   \n",
    "                    # see nltk.help.upenn_tagset() for parts of speech\n",
    "                    # leaving in these tokens does not change accuracy but does increase average distance by 1\n",
    "                    # removing all parts of speech reduces the accuracy and increases the average distance, indicating model relies on pos tag\n",
    "                    emb.append(torch.cat((tkn[0],tkn[1])))\n",
    "\n",
    "            embeddings.append(emb)\n",
    "\n",
    "            # uncomment, rm emb.append(torch.cat((tkn[0],tkn[1]))) to include all tokens and unmapped pos as 0-vector\n",
    "#             embeddings.append([torch.cat((tkn[0],tkn[1])) for tkn in embedding])\n",
    "            targets.append(target)\n",
    "    \n",
    "    if pad: embeddings = pad_minibatch(embeddings)\n",
    "# return target vals for accuracy \n",
    "    return embeddings, num_vectorize(targets), torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c\n",
    "# transform proba dist prediction to MMSE target\n",
    "def out_to_score_proba(yhat_vector):\n",
    "    # mod to 31 for ix 0-30\n",
    "    proba_vector = torch.zeros([31])\n",
    "    # no zero-score\n",
    "    proba_vector[0] = 0. \n",
    "# P(MMSE=i) = P(MMSE>i-1) - P(MMSE>i)\n",
    "    for i in range(1,len(yhat_vector)):\n",
    "        proba_vector[i] = yhat_vector[i-1] - yhat_vector[i]\n",
    "        \n",
    "    proba_vector[-1] = yhat_vector[-1] \n",
    "    \n",
    "#     print(proba_vector)\n",
    "    return torch.argmax(proba_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not on y but on targets as mmse scores\n",
    "def accuracy_distansum(yhat,y):\n",
    "    return (torch.abs(y - yhat).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw accurate prediction count\n",
    "def accuracy(yhat_tensor,y_tensor):\n",
    "#     return torch.stack([ya==yb for (ya,yb) in zip(yhat_tensor,y_tensor)]).sum()\n",
    "    return ((yhat_tensor == y_tensor).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-layer rnn: gru units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "# EMBEDDING_SIZE = 371\n",
    "EMBEDDING_SIZE = len(vocab_emb['I']) + len(pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max([len(emb) for emb in e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change if aadd in EOS, pos to 547\n",
    "MAX_LENGTH = 546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size=MAX_LENGTH, emb_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size=self.emb_size,hidden_size=self.hidden_size,num_layers=2).double()\n",
    "#         # each output node Oi of our neural network\n",
    "# uses a standard sigmoid function 1\n",
    "# 1+e−zi\n",
    "# , without including\n",
    "# the outputs from other nodes, as shown in Figure 1. Output\n",
    "# node Oi\n",
    "# is used to estimate the probability oi\n",
    "# that a data\n",
    "# point belongs to category i independently, without subjecting\n",
    "# to normalization as traditional neural networks do. Thus,\n",
    "# for a data point x of category k, the target vector is\n",
    "# (1, , 1, .., 1, 0, 0, 0), in which the first k elements is 1 and\n",
    "# others 0\n",
    "# http://orca.st.usm.edu/~zwang/files/rank.pdf A Neural Network Approach to Ordinal Regression\n",
    "    \n",
    "        self.fc = nn.Linear(self.hidden_size*2,30).double()\n",
    "        \n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        embedded = input.double()\n",
    "\n",
    "        gru_out, _ = self.gru(embedded,hidden)\n",
    "        \n",
    "        hid_out = torch.cat((_[-2,:,:], _[-1,:,:]), dim = 1)\n",
    "\n",
    "        out = self.fc(hid_out)\n",
    "#         print(out.size())\n",
    "\n",
    "        out - self.activation(out)\n",
    "\n",
    "        return out, _\n",
    "#         return torch.nn.utils.rnn.pad_packed_sequence(out), _\n",
    "\n",
    "    def initHidden(self,batch_size=BATCH_SIZE):\n",
    "        # h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2\n",
    "        return torch.zeros(2, batch_size, self.hidden_size, device=device).double()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The\n",
    "# cost function for a data point x can be relative entropy\n",
    "# or square error between the target vector and the output\n",
    "# vector\n",
    "# loss = nn.BCELoss()\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load('./encoder_0.99accuracy_25epch.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (gru): GRU(371, 128, num_layers=2)\n",
       "  (fc): Linear(in_features=256, out_features=30, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trace,y_trace,targets_trace = get_embs(mem_ix=list(y_mem.keys()),tech_ix=list(y_tech.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_mem) + len(y_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hidden = encoder.initHidden(batch_size=len(y_mem)+len(y_tech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat, _hidden = encoder(x_trace, _hidden)\n",
    "# Rm non-pos\n",
    "estimates = torch.tensor([out_to_score_proba(hat) for hat in yhat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9/96 accurate mmse prediction\n",
    "accuracy(targets_trace,estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avg mmse distance from target\n",
    "# Any score of 24 or more (out of 30) indicates a normal cognition. Below this, scores can indicate severe (≤9 points), moderate (10–18 points) or mild (19–23 points) cognitive impairment.\n",
    "# Crum RM, Anthony JC, Bassett SS, Folstein MF; Anthony; Bassett; Folstein (May 1993). \"Population-based norms for the Mini-Mental Status Examination by age and educational level\". JAMA. 269 (18): 2386–91. doi:10.1001/jama.1993.03500180078038. PMID 8479064.\n",
    "# https://en.wikipedia.org/wiki/Mini%E2%80%93Mental_State_Examination#:~:text=Any%20score%20of%2024%20or,%E2%80%9323%20points)%20cognitive%20impairment.\n",
    "accuracy_distansum(y=targets_trace,yhat=estimates)/len(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29, 29, 28, 28, 23, 28, 29, 23, 16, 29, 28, 16,  3, 28, 28, 29, 30, 29,\n",
       "        19, 28, 28, 29, 28, 29, 28, 23, 16, 24, 20, 29, 29, 13, 29, 28, 29, 28,\n",
       "        23, 30, 20, 28, 26, 28, 28, 28, 23, 28, 23, 28,  5, 19, 19, 29, 19, 20,\n",
       "        28, 19, 28, 28, 28, 20, 29, 30, 28, 28, 23, 19, 19, 28, 18, 17, 28, 28,\n",
       "        25, 29, 28, 20, 28, 20, 19, 16, 19, 28, 23, 28, 28, 20, 28, 19, 28, 28,\n",
       "        28, 23, 21, 29, 29, 28])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 30, 28, 30, 30, 30, 30, 29, 29, 29, 30, 30, 30, 29, 30, 30, 30, 30,\n",
       "        28, 29, 30, 28, 30, 29, 30, 26, 30, 29, 29, 30, 30, 29, 30, 28, 30, 30,\n",
       "        30, 30, 30, 30, 30, 30, 29, 30, 30, 29, 28, 30, 28, 30, 30, 30, 30, 29,\n",
       "        29, 29, 30, 30, 30, 29, 29, 30, 30, 30, 30, 30, 28, 29, 30, 28, 28, 30,\n",
       "        29, 30, 26, 30, 29, 29, 30, 30, 29, 30, 28, 30, 30, 30, 30, 30, 30, 30,\n",
       "        30, 29, 30, 30, 30, 29])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09375"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9/96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memory samples -- accuracy 6/46 (.13)\n",
    "accuracy(targets_trace[:46],estimates[:46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(accuracy_distansum(y=targets_trace[:46],yhat=estimates[:46]))/len(estimates[:46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tech samples -- accuracy 3/50 (.06)\n",
    "accuracy(targets_trace[46:],estimates[46:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(accuracy_distansum(y=targets_trace[46:],yhat=estimates[46:]))/len(targets_trace[46:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29, 30, 28, 28, 23, 29, 29, 23, 16, 29, 28, 16,  3, 28, 28, 29, 28, 19,\n",
       "        19, 28, 28, 29, 28, 29, 28, 23, 16, 24, 20, 29, 28, 13, 29, 28, 29, 28,\n",
       "        23, 30, 20, 28, 26, 28, 28, 28, 23, 28, 23, 16,  5, 19, 13, 29, 19, 11,\n",
       "        28, 19, 19,  3, 28, 28, 29, 30, 19, 28, 23, 19, 14, 28, 18, 17, 28, 28,\n",
       "        16, 29, 28, 15, 28, 20,  3, 16, 19, 28, 23, 24, 28, 20, 28, 19, 28, 28,\n",
       "        28, 23, 21, 29, 29, 28])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using all tokens, pos including nonmapped pos\n",
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.abs(targets_trace - estimates).sum())/len(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29, 29, 20, 28, 23, 29, 29, 23, 16, 29, 28, 16, 19, 28, 28, 29, 28, 19,\n",
       "        19, 25, 28, 29, 17, 29, 28, 23, 16, 24, 20, 29, 17, 29, 29, 28, 29, 28,\n",
       "        23, 30, 29, 28, 26, 28, 28, 19, 14, 28, 23, 16,  5, 25, 13, 29, 19, 19,\n",
       "        25, 19, 19,  3, 28, 20, 25, 30, 19, 29, 26, 19, 14, 28, 18, 17, 20, 28,\n",
       "        16, 29, 28, 15, 28, 20,  5, 16, 19, 20, 23, 24, 28, 20, 13, 19, 16, 28,\n",
       "        28, 23, 28, 19, 29, 28])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without pos tags - ix 13 'I have been able to recall - in detail -- memories from my first year.  This was confirmed by my Mother.' jumps from 3 to 19 \n",
    "# ...but pos is imp...tweak pos output?\n",
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distance without pos tags (all pos replaced by 0-vector)\n",
    "(torch.abs(targets_trace - estimates).sum())/len(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 30, 28, 30, 30, 30, 30, 29, 29, 29, 30, 30, 30, 29, 30, 30, 30, 30,\n",
       "        28, 29, 30, 28, 30, 29, 30, 26, 30, 29, 29, 30, 30, 29, 30, 28, 30, 30,\n",
       "        30, 30, 30, 30, 30, 30, 29, 30, 30, 29, 28, 30, 28, 30, 30, 30, 30, 29,\n",
       "        29, 29, 30, 30, 30, 29, 29, 30, 30, 30, 30, 30, 28, 29, 30, 28, 28, 30,\n",
       "        29, 30, 26, 30, 29, 29, 30, 30, 29, 30, 28, 30, 30, 30, 30, 30, 30, 30,\n",
       "        30, 29, 30, 30, 30, 29])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for heatmap visualization\n",
    "pd.Series(targets_trace).to_csv('targets_trace.csv')\n",
    "\n",
    "pd.Series(estimates).to_csv('estimates_trace.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([158, 30])\n"
     ]
    }
   ],
   "source": [
    "# compare performance with eval on validation addb set -- .99 accuracy \n",
    "encoder.eval()\n",
    "\n",
    "x_valid,y_valid,targets_valid = get_minibatch(ix=valid_ix)\n",
    "\n",
    "# x_valid = pad_minibatch(x_valid)\n",
    "\n",
    "# y_valid = torch.stack(y_valid).double()\n",
    "\n",
    "_hidden = encoder.initHidden(batch_size=len(valid_ix))\n",
    "\n",
    "yhat_valid, _hidden = encoder(x_valid, _hidden)\n",
    "estimates = torch.tensor([out_to_score_proba(hat) for hat in yhat_valid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 18, 29, 28, 26, 27, 23, 30, 19, 17, 20, 17, 25, 30, 23, 29, 27, 30,\n",
       "        19, 16, 30,  7, 28, 28, 28, 28, 25, 12, 30, 18, 30, 25, 15, 18, 14, 26,\n",
       "        27, 30, 20, 17, 30, 26, 19, 27, 17, 20, 20, 29, 27, 29, 30, 30, 30, 20,\n",
       "        22, 29, 19, 30, 30, 30, 30, 22, 17, 27, 27, 30, 30, 23, 30, 30, 19, 25,\n",
       "        30, 19, 30, 30, 30,  8, 30, 22, 30, 30, 28, 16, 19, 22, 24, 28, 16, 30,\n",
       "        30, 19, 17, 27, 19, 30, 30, 21, 28, 19, 29, 19, 12, 28, 20, 23, 13, 20,\n",
       "        23, 27, 23, 19, 20, 19, 19, 19, 27, 16, 10, 20, 17, 24, 24, 23, 16, 20,\n",
       "        18, 17, 19, 28, 29, 20, 27, 20, 23, 16, 20, 23, 27, 22, 19, 20, 20, 19,\n",
       "        19, 28, 28, 28, 26, 19, 24, 22, 20, 18, 23, 27, 25, 25])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 18, 29, 30, 26, 27, 23, 30, 19, 17, 20, 17, 25, 30, 23, 29, 27, 30,\n",
       "        19, 16, 30,  7, 28, 28, 28, 28, 25, 12, 30, 18, 30, 25, 15, 18, 14, 26,\n",
       "        27, 30, 20, 17, 30, 26, 19, 27, 17, 20, 20, 29, 27, 29, 30, 30, 30, 20,\n",
       "        22, 29, 19, 30, 30, 30, 30, 22, 17, 27, 27, 30, 30, 23, 30, 30, 19, 25,\n",
       "        30, 19, 30, 30, 30,  8, 30, 22, 30, 30, 28, 16, 19, 22, 24, 28, 16, 30,\n",
       "        30, 19, 17, 27, 19, 30, 30, 21, 28, 19, 29, 19, 12, 28, 20, 23, 13, 20,\n",
       "        23, 27, 23, 19, 20, 19, 19, 19, 27, 16, 10, 20, 17, 24, 24, 23, 16, 20,\n",
       "        18, 17, 19, 28, 29, 20, 27, 20, 23, 16, 20, 23, 27, 22, 19, 20, 20, 19,\n",
       "        19, 28, 28, 28, 26, 19, 24, 22, 20, 18, 23, 27, 25, 25])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(157)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(targets_valid,estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9936708860759493"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "157/158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3]),)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(targets_valid != estimates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python dl1010",
   "language": "python",
   "name": "dl1010"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
