{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylangacq as pla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_cookie = torch.load('cookie_data_targets.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/renee/Documents/CT_Fa18/Spec/Trace/FinalProj'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mem = torch.load('mem_mmse_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tech = torch.load('tech_mmse_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([6, 29, 51, 56, 78, 89, 100, 107, 118, 154, 184, 202, 217, 226, 234, 242, 266, 277, 280, 286, 297, 303, 324, 337, 347, 352, 359, 375, 387, 398, 411, 423, 433, 438, 445, 448, 460, 464, 470, 474, 477, 496, 499, 502, 506, 509, 519, 522, 537, 540])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tech.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_datasheet = pd.read_excel('CogData_FU_82818.xlsx',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                             Record ID\n",
       "1                                            Event Name\n",
       "2     Write a short sketch about a memory from your ...\n",
       "3     How does technology and social media impact th...\n",
       "4                   Date of Neurobehavioral Status Exam\n",
       "                            ...                        \n",
       "92                         Cog State: One Back Accuracy\n",
       "93                          Ravens Progressive Matrices\n",
       "94                                Logical Memory Part B\n",
       "95                         East Boston Immediate Recall\n",
       "96                                            Complete?\n",
       "Name: 0, Length: 97, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apc_datasheet.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6      When I was a little child I had the same exper...\n",
       "29     When I was very young, living in the Astoria n...\n",
       "51     I went to Paris when I was 16.  I was staying ...\n",
       "56     My husband and I had always wanted to go to Pa...\n",
       "78     I was born in Hungary. A significant memory is...\n",
       "89     When I was 16 years old I traveled to Europe b...\n",
       "100    I have great memories of growing up in huge ho...\n",
       "107    When I was 8 or 9 years old, I was playing on ...\n",
       "118    One of my fondest childhood memories is how mu...\n",
       "154    One of my favorite memories from my childhood ...\n",
       "184    My favorite pastime as a small child was re-en...\n",
       "202    I remember going to the park with my dad to pl...\n",
       "217    At the age of 14, I was the setter on a volley...\n",
       "226    I have been able to recall - in detail -- memo...\n",
       "242    I have a very clear memeory of last time I saw...\n",
       "266    My parents lived in Ft. Myers, Florida, when I...\n",
       "277    i visited Washington DC with my brother, my gr...\n",
       "286    I remember finally going to the country in the...\n",
       "297    My dad took my friends and cousins  and me cam...\n",
       "303    A vivid memory of my childhood was about my fa...\n",
       "324    My dad was off on Wednesday's and we often wen...\n",
       "347         My earliest memory is standing, in diaper...\n",
       "352    We took very few trips as a family. We had lim...\n",
       "359    They say traumatic memories tend to remain viv...\n",
       "375    I remember that in fourth grade, there was an ...\n",
       "387    My mother frequently threatened to kill hersel...\n",
       "398    I had auditioned for a repertory company in my...\n",
       "411    learning to ride a bike at the jersey shore on...\n",
       "423    In fifth grade my grandparents took my sister ...\n",
       "433    I have fond memories of going to TOBAY beach o...\n",
       "438    I was 13, & 8 of our 9 family members set off ...\n",
       "445    When I was seven my father went off to play te...\n",
       "448    Two memories come to mind in response to this ...\n",
       "460    I remember how we celebrated the Jewish holida...\n",
       "464    I spent a lot of time as an only child growing...\n",
       "470    I remember riding on my father's shoulders.  I...\n",
       "474    My family went on a vacation to Hawaii, Oahu. ...\n",
       "477    When I was young we used to take one family va...\n",
       "496             This is too much pressure for a writer. \n",
       "499    My mother's best friend drove a convertible.  ...\n",
       "502       When I visited relatives in Alabama, I enjo...\n",
       "506    When I was in 8th grade I flew to Sea Island G...\n",
       "509    I loved growing up in Israel and being in a sa...\n",
       "522    This is my kids favorite story about what it w...\n",
       "537    When I was young I went to a sleep away camp e...\n",
       "540    My father was in charge of building the backpa...\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apc_datasheet.iloc[list(y_mem.keys()),2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6      I stay in touch with family and friends throug...\n",
       "29     I'm a journalist who has covered the rise of t...\n",
       "51     My husband and I are on our computers all day ...\n",
       "56     I'm on the computer all the time, writing or r...\n",
       "78     My use of social media is minimal. I only use ...\n",
       "89     I check that on what my children are doing and...\n",
       "100    Technology has helped me connect to family and...\n",
       "107    I use a laptop and phone constantly for work a...\n",
       "118    I do not use any social media sites at this ti...\n",
       "154    Social media is great to stay in touch with wh...\n",
       "184    Social Media is a bit of a challenge as I am a...\n",
       "202    I use email daily for work and it usually cont...\n",
       "217    Technology is woven into the fabric of our liv...\n",
       "226    We dont engage on social media. Email exchange...\n",
       "234    I'm writing correspondence via computer most o...\n",
       "242    If by Technology we refer  to communication te...\n",
       "266    Social media not much, since I don'tuse them, ...\n",
       "277    I work in a business that is driven by technol...\n",
       "280                   Does not affect my life or family.\n",
       "286    I use my smart phone often in my iPad and try ...\n",
       "297    It is ever present although i see it most prob...\n",
       "303    Since I worked up until 2013, I had adjusted t...\n",
       "324                       It's easy to be on it too much\n",
       "337    Technology has interrupted my personal life th...\n",
       "347         Technology and social media have had huge...\n",
       "352    Technology and the internet give us access to ...\n",
       "359    Technology and social media is a double edged ...\n",
       "375    Technology gives us access to an unbelievable ...\n",
       "387    It is both a help and a hindrance because it i...\n",
       "398    It keeps me in the know about people (friends ...\n",
       "411    Provides an efficient way to gain access to re...\n",
       "423    Don't care for social media. Although I have a...\n",
       "433    I don't use social media, as I hear that it co...\n",
       "438    Good: keep up with friends & family, exposure ...\n",
       "445    Technology and social media have become part o...\n",
       "448    At this time in my life, technology has just b...\n",
       "460    I find technology a wonderful way for me to st...\n",
       "464    I think social media is incredibly distracting...\n",
       "470    Two steps forward, one back.  Technology is am...\n",
       "474    I have a Facebook account, but I only subscrib...\n",
       "477    I spend all day at the beck and call of my sma...\n",
       "496    Okay... I'll combine a bit of question 1 with ...\n",
       "499    I use a computer for emails, work, research an...\n",
       "502    Technology is a time killer that speeds everyt...\n",
       "506    I use my computer or tablet and my phone daily...\n",
       "509    I try not to spend time on facebook--I don't f...\n",
       "519    I feel that a lot of times technology and soci...\n",
       "522    I'm glad I had my children fairly late, becaus...\n",
       "537    Technology is a very important part of my life...\n",
       "540    Technology impacts me and my family on a daily...\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apc_datasheet.iloc[list(y_tech.keys()),3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Vocab Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = './FinalProj/PretrainedWordEmb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/renee/Documents/CT_Fa18/Spec/Trace'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_emb = pickle.load(open(f'{glove_path}/trace.vocab.glove.42B.300_words.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2125"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_addb_posdict = torch.load('apc_pos_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CD': 'DET:NUM',\n",
       " 'FW': 'FW',\n",
       " 'EX': 'PRO:EXIST',\n",
       " 'WP': 'PRO:INT',\n",
       " 'NNS': 'N',\n",
       " 'WRB': 'ADV',\n",
       " '(': '(',\n",
       " 'RB': 'ADV',\n",
       " 'PDT': 'DET:DEM',\n",
       " 'VBG': 'PART',\n",
       " 'CC': 'CONJ',\n",
       " \"''\": \"''\",\n",
       " ':': ':',\n",
       " 'VBP': 'V',\n",
       " 'IN': 'PREP',\n",
       " 'TO': 'PREP',\n",
       " 'MD': 'MOD',\n",
       " 'PRP$': 'PRO:POSS',\n",
       " ')': ')',\n",
       " 'PRP': 'PRO:PER',\n",
       " 'JJ': 'ADJ',\n",
       " '.': '.',\n",
       " 'JJR': 'ADJ',\n",
       " 'VBD': 'V',\n",
       " 'RBR': 'ADV',\n",
       " 'RP': 'PART',\n",
       " ',': ',',\n",
       " 'VB': 'V',\n",
       " 'VBN': 'PART',\n",
       " 'NN': 'N',\n",
       " 'NNPS': 'N:PROP',\n",
       " 'WDT': 'DET:POSS',\n",
       " 'POS': 'DET:POSS',\n",
       " '$': '$',\n",
       " 'NNP': 'N:PROP',\n",
       " 'JJS': 'ADJ',\n",
       " 'VBZ': 'V',\n",
       " 'RBS': 'ADV',\n",
       " 'DT': 'DET:ART'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_addb_posdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = torch.load('./FinalProj/pos_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'CM': 1,\n",
       " 'COMP': 2,\n",
       " 'PRO:EXIST': 3,\n",
       " 'IN#ADJ': 4,\n",
       " 'PRE#PART': 5,\n",
       " '.': 6,\n",
       " 'PRE#V': 7,\n",
       " 'OVER#PART': 8,\n",
       " 'UN#ADV': 9,\n",
       " 'PRO:PER': 10,\n",
       " 'ADV:TEM': 11,\n",
       " '+/.': 12,\n",
       " 'V': 13,\n",
       " 'PRO:INT': 14,\n",
       " 'N': 15,\n",
       " 'PRO:POSS': 16,\n",
       " 'N:ADJ': 17,\n",
       " 'DET:DEM': 18,\n",
       " 'END': 19,\n",
       " 'PRO:INDEF': 20,\n",
       " '+\"/.': 21,\n",
       " 'DET:POSS': 22,\n",
       " 'OVER#V': 23,\n",
       " 'DET:ART': 24,\n",
       " 'INF': 25,\n",
       " 'OVER#N:GERUND': 26,\n",
       " 'META': 27,\n",
       " 'ON': 28,\n",
       " 'UP#V': 29,\n",
       " 'MID#N': 30,\n",
       " 'NEG': 31,\n",
       " 'MOD': 32,\n",
       " '+...': 33,\n",
       " 'PRO:SUB': 34,\n",
       " 'UN#PART': 35,\n",
       " 'PREP': 36,\n",
       " 'DET:NUM': 37,\n",
       " 'N:GERUND': 38,\n",
       " '+//?': 39,\n",
       " 'UN#ADJ': 40,\n",
       " 'COP': 41,\n",
       " 'N:PT': 42,\n",
       " 'AUX': 43,\n",
       " 'ADV': 44,\n",
       " 'OUT#PART': 45,\n",
       " '!': 46,\n",
       " 'PRO:OBJ': 47,\n",
       " 'ADJ': 48,\n",
       " 'N:PROP': 49,\n",
       " 'MINI#N': 50,\n",
       " '+/?': 51,\n",
       " '?': 52,\n",
       " 'POST': 53,\n",
       " 'N:LET': 54,\n",
       " 'UN#N': 55,\n",
       " 'UP#PART': 56,\n",
       " 'COORD': 57,\n",
       " 'QN': 58,\n",
       " 'GRAND#N': 59,\n",
       " 'CONJ': 60,\n",
       " '+..?': 61,\n",
       " 'PART': 62,\n",
       " 'BEG': 63,\n",
       " 'PRO:REL': 64,\n",
       " 'PRO:REFL': 65,\n",
       " 'NEO': 66,\n",
       " 'CO': 67,\n",
       " '+\".': 68,\n",
       " 'PRO:DEM': 69,\n",
       " '+//.': 70}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group minibatches of similar size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file index: sequence length\n",
    "ix_size_dict = {}\n",
    "minibatch_ix = train_ix\n",
    "    \n",
    "#index, filename\n",
    "cookie_files = [(i,X_cookie_filenames[i]) for i in minibatch_ix if i in X_cookie_filenames.keys()]\n",
    "sent_files = [(i,X_sent_filenames[i] )for i in minibatch_ix if i in X_sent_filenames.keys()]\n",
    "\n",
    "\n",
    "for corpus,data,targetdict in [(cookie_files,X_cookie_reader,y_cookie),(sent_files,X_sentence_reader,y_sentence)]: \n",
    "    for file_ix,file in corpus:\n",
    "#             print(file_ix, file)\n",
    "#             print('Words',[tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance])\n",
    "#             print('Words_len',len([tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance]))\n",
    "        embedding = [(vocab_emb[token],torch.zeros(len(pos_dict),dtype=torch.float64),pos_dict[pos]) for (token,pos) in zip([tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance], [pos for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance])]\n",
    "        ix_size_dict[file_ix] = len(embedding)\n",
    "#         target = targetdict[file_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length: [file indices]\n",
    "size_ix_dict = {}\n",
    "for k,v in ix_size_dict.items():\n",
    "    if v not in size_ix_dict.keys():\n",
    "        size_ix_dict[v] = [k]\n",
    "    else:\n",
    "        size_ix_dict[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of sequence-lengths\n",
    "len_sents = sorted(list(size_ix_dict.keys()),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-07747f6bed1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatchset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mminibsize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mbatchset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_ix_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mminibsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlen_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "batches = []\n",
    "while len(len_sents) > 0:\n",
    "    minibsize = 0\n",
    "    batchset = []\n",
    "    while minibsize < 25:\n",
    "        batchset.extend(size_ix_dict[len_sents[0]])\n",
    "        minibsize = len(batchset)\n",
    "        len_sents.pop(0)\n",
    "    batches.append(batchset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append final batchset\n",
    "batches.append(batchset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm all train files accounted for \n",
    "sum([1 for blist in batches for bitem in blist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Write a short sketch about a memory from your ...\n",
       "1                                                    NaN\n",
       "2                                                    NaN\n",
       "3                                                    NaN\n",
       "4                                                    NaN\n",
       "                             ...                        \n",
       "572                                                  NaN\n",
       "573                                                  NaN\n",
       "574                                                  NaN\n",
       "575                                                  NaN\n",
       "576                                                  NaN\n",
       "Name: 2, Length: 577, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apc_datasheet[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad with zeros -- maybe\n",
    "# end with eos tensor, tag\n",
    "\n",
    "def get_embs(batchsize=1, mem_ix=[],tech_ix=[],pad=True):\n",
    "    \n",
    "    def list_tokenize(document):\n",
    "#     return re.sub('['+string.punctuation.replace('\\'','')+']',' ',response).split()\n",
    "        sentences = nltk.sent_tokenize(document)\n",
    "\n",
    "        sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "        return [token for sentence in sentences for token in sentence]\n",
    "    \n",
    "    def get_pos(document):\n",
    "        #sentence segmentation\n",
    "        sentences = nltk.sent_tokenize(document) \n",
    "        #tokenization\n",
    "        sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "        #pos tagging\n",
    "        sentences = [nltk.pos_tag(sent) for sent in sentences] \n",
    "\n",
    "        pos_list = [p for sent in sentences for (t,p) in sent]\n",
    "        return pos_list\n",
    "\n",
    "    def num_vectorize(targets):\n",
    "        vectors = []\n",
    "        for i in targets:\n",
    "            v = torch.zeros([30])\n",
    "            # up until MMSE index...greater than index i \n",
    "            v[:i] = 1\n",
    "            vectors.append(v)\n",
    "        return torch.stack(vectors).double()\n",
    "    \n",
    "    def pad_minibatch(minib):\n",
    "    \n",
    "        batchsize = len(minib)\n",
    "\n",
    "        seq_lens = [len(emb) for emb in minib]\n",
    "        max_len = max(seq_lens)\n",
    "\n",
    "        # input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "    #     seq_tensor = torch.zeros((batchsize, max_len, EMBEDDING_SIZE)).double()\n",
    "        seq_tensor = torch.zeros((max_len, batchsize, EMBEDDING_SIZE)).double()\n",
    "\n",
    "\n",
    "        for i, (seq,length) in enumerate( zip(minib,seq_lens) ):\n",
    "            for wi,word in enumerate(seq):\n",
    "    #             seq_tensor[i,wi] = word\n",
    "                seq_tensor[wi,i] = word\n",
    "        # mod to sort at the start?\n",
    "        seq_tensor = nn.utils.rnn.pack_padded_sequence(seq_tensor,lengths=seq_lens,enforce_sorted=False)\n",
    "        return seq_tensor\n",
    "    \n",
    "#     minibatch_ix = random.sample(range(num_samples),batchsize) if ix is None else ix \n",
    "    \n",
    "    #index, filename\n",
    "#     cookie_files = [(i,X_cookie_filenames[i]) for i in minibatch_ix if i in X_cookie_filenames.keys()]\n",
    "#     sent_files = [(i,X_sent_filenames[i] )for i in minibatch_ix if i in X_sent_filenames.keys()]\n",
    "    \n",
    "    embeddings = []\n",
    "    targets = []\n",
    "\n",
    "\n",
    "    for ix_list,data,targetdict in [(mem_ix,apc_datasheet[2],y_mem),(tech_ix,apc_datasheet[3],y_tech)]:\n",
    "        \n",
    "        for ix in ix_list:\n",
    "#             print('ix',ix,'data',data.iloc[ix],'targetdict',targetdict)\n",
    "#             return data.iloc[ix]\n",
    "            embedding = [(vocab_emb[token],torch.zeros(len(pos_dict),dtype=torch.float64),trace_addb_posdict[pos]) for (token,pos) in zip(list_tokenize(data.iloc[ix]), get_pos(data.iloc[ix]))]\n",
    "            \n",
    "#             print('embedding',embedding)\n",
    "            target = targetdict[ix]\n",
    "  # RM POS test  \n",
    "            emb = []\n",
    "            for tkn in embedding:\n",
    "                if tkn[2] in pos_dict:\n",
    "                    tkn[1][pos_dict[tkn[2]]] = 1\n",
    "                    # parts of speech ['FW', '(', \"''\", ':', ')', ',', '$'] not in addb dataset -- rm corresponding tokens  \n",
    "                    # see nltk.help.upenn_tagset() for parts of speech\n",
    "                    # leaving in these tokens does not change accuracy but does increase average distance by 1\n",
    "                    # removing all parts of speech reduces the accuracy and increases the average distance, indicating model relies on pos tag\n",
    "                    emb.append(torch.cat((tkn[0],tkn[1])))\n",
    "                    \n",
    "#                 else:\n",
    "                    \n",
    "#                     print(tkn[2] +' not in pos_dict')\n",
    "\n",
    "    #                 embeddings.append(torch.cat((tkn[0],tkn[1])))\n",
    "            embeddings.append(emb)\n",
    "#             embeddings.append([torch.cat((tkn[0],tkn[1])) for tkn in embedding])\n",
    "            targets.append(target)\n",
    "#             print(embeddings)\n",
    "#             print(targets)\n",
    "#     for corpus,data,targetdict in [(cookie_files,X_cookie_reader,y_cookie),(sent_files,X_sentence_reader,y_sentence)]: \n",
    "#         for file_ix,file in corpus:\n",
    "# #             print(file_ix, file)\n",
    "# #             print('Words',[tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance])\n",
    "# #             print('Words_len',len([tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance]))\n",
    "#             embedding = [(vocab_emb[token],torch.zeros(len(pos_dict),dtype=torch.float64),pos_dict[pos]) for (token,pos) in zip([tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance], [pos for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance])]\n",
    "#             target = targetdict[file_ix]\n",
    "            \n",
    "#             for tkn in embedding:\n",
    "\n",
    "#                 tkn[1][tkn[2]] = 1\n",
    "\n",
    "# #                 embeddings.append(torch.cat((tkn[0],tkn[1])))\n",
    "            \n",
    "#             embeddings.append([torch.cat((tkn[0],tkn[1])) for tkn in embedding])\n",
    "\n",
    "\n",
    "# #             embeddings.append(embedding)\n",
    "#             targets.append(target)\n",
    "#     print(targets)       \n",
    "#     return embeddings, torch.tensor(targets), minibatch_ix\n",
    "#     return embeddings, torch.tensor(targets)\n",
    "    if pad: embeddings = pad_minibatch(embeddings)\n",
    "# return target vals for accuracy \n",
    "    return embeddings, num_vectorize(targets), torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 29, 51, 56, 78]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_tech.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_tokenize(document):\n",
    "#     return re.sub('['+string.punctuation.replace('\\'','')+']',' ',response).split()\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    return [token for sentence in sentences for token in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'I',\n",
       " 'was',\n",
       " 'a',\n",
       " 'little',\n",
       " 'child',\n",
       " 'I',\n",
       " 'had',\n",
       " 'the',\n",
       " 'same',\n",
       " 'experience',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " '.',\n",
       " 'It',\n",
       " 'always',\n",
       " 'occurred',\n",
       " 'between',\n",
       " 'being',\n",
       " 'awake',\n",
       " 'and',\n",
       " 'asleep',\n",
       " '.',\n",
       " 'The',\n",
       " 'experience',\n",
       " 'had',\n",
       " 'no',\n",
       " 'words',\n",
       " '...',\n",
       " 'it',\n",
       " 'was',\n",
       " 'all',\n",
       " 'sensation',\n",
       " '.',\n",
       " 'At',\n",
       " 'a',\n",
       " 'point',\n",
       " 'between',\n",
       " 'my',\n",
       " 'eyes',\n",
       " 'I',\n",
       " 'got',\n",
       " 'a',\n",
       " 'feeling',\n",
       " 'of',\n",
       " 'all',\n",
       " 'space',\n",
       " 'expanding',\n",
       " 'and',\n",
       " 'expanding',\n",
       " 'until',\n",
       " 'it',\n",
       " 'became',\n",
       " 'so',\n",
       " 'vast',\n",
       " 'I',\n",
       " 'was',\n",
       " 'terrified',\n",
       " '.',\n",
       " 'Then',\n",
       " 'all',\n",
       " 'space',\n",
       " 'would',\n",
       " 'contract',\n",
       " 'until',\n",
       " 'it',\n",
       " 'became',\n",
       " 'so',\n",
       " 'tiny',\n",
       " 'I',\n",
       " 'would',\n",
       " 'be',\n",
       " 'terrified',\n",
       " '.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'both',\n",
       " 'afraid',\n",
       " 'of',\n",
       " 'having',\n",
       " 'this',\n",
       " 'experience',\n",
       " 'and',\n",
       " 'welcomed',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tokenize(apc_datasheet[2].iloc[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "e,_,t = get_embs(tech_ix=list(y_tech.keys())[:1],mem_ix=list(y_mem.keys())[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97, 371])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 28])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2964, -0.3809, -0.3653,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0435, -0.1848, -0.1461,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.1204,  0.0567, -0.4998,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.2868,  0.1821, -0.0600,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2773, -0.1023,  0.1760,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0068, -0.0024,  0.0248,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c\n",
    "# transform proba dist prediction to MMSE target\n",
    "def out_to_score_proba(yhat_vector):\n",
    "    # mod to 31 for ix 0-30\n",
    "    proba_vector = torch.zeros([31])\n",
    "    # no zero-score\n",
    "    proba_vector[0] = 0. \n",
    "# P(MMSE=i) = P(MMSE>i-1) - P(MMSE>i)\n",
    "    for i in range(1,len(yhat_vector)):\n",
    "        proba_vector[i] = yhat_vector[i-1] - yhat_vector[i]\n",
    "        \n",
    "    proba_vector[-1] = yhat_vector[-1] \n",
    "    \n",
    "#     print(proba_vector)\n",
    "    return torch.argmax(proba_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not on y but on targets as mmse scores\n",
    "def accuracy_distansum(yhat_tensor,y_tensor):\n",
    "    return (torch.abs(y - yhat).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw accurate prediction count\n",
    "def accuracy(yhat_tensor,y_tensor):\n",
    "#     return torch.stack([ya==yb for (ya,yb) in zip(yhat_tensor,y_tensor)]).sum()\n",
    "    return ((yhat_tensor == y_tensor).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-layer rnn: gru units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "# EMBEDDING_SIZE = 371\n",
    "EMBEDDING_SIZE = len(vocab_emb['I']) + len(addb_pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max([len(emb) for emb in e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change if aadd in EOS, pos to 547\n",
    "MAX_LENGTH = 546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size=MAX_LENGTH, emb_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size=self.emb_size,hidden_size=self.hidden_size,num_layers=2).double()\n",
    "#         # each output node Oi of our neural network\n",
    "# uses a standard sigmoid function 1\n",
    "# 1+e−zi\n",
    "# , without including\n",
    "# the outputs from other nodes, as shown in Figure 1. Output\n",
    "# node Oi\n",
    "# is used to estimate the probability oi\n",
    "# that a data\n",
    "# point belongs to category i independently, without subjecting\n",
    "# to normalization as traditional neural networks do. Thus,\n",
    "# for a data point x of category k, the target vector is\n",
    "# (1, , 1, .., 1, 0, 0, 0), in which the first k elements is 1 and\n",
    "# others 0\n",
    "# http://orca.st.usm.edu/~zwang/files/rank.pdf A Neural Network Approach to Ordinal Regression\n",
    "    \n",
    "        self.fc = nn.Linear(self.hidden_size*2,30).double()\n",
    "        \n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        embedded = input.double()\n",
    "\n",
    "        gru_out, _ = self.gru(embedded,hidden)\n",
    "        \n",
    "        hid_out = torch.cat((_[-2,:,:], _[-1,:,:]), dim = 1)\n",
    "\n",
    "        out = self.fc(hid_out)\n",
    "#         print(out.size())\n",
    "\n",
    "        out - self.activation(out)\n",
    "\n",
    "        return out, _\n",
    "#         return torch.nn.utils.rnn.pad_packed_sequence(out), _\n",
    "\n",
    "    def initHidden(self,batch_size=BATCH_SIZE):\n",
    "        # h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2\n",
    "        return torch.zeros(2, batch_size, self.hidden_size, device=device).double()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The\n",
    "# cost function for a data point x can be relative entropy\n",
    "# or square error between the target vector and the output\n",
    "# vector\n",
    "# loss = nn.BCELoss()\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25\n",
      "===========\n",
      "torch.Size([26, 30])\n",
      "step 1\n",
      "loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(31.8367, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([13, 30])\n",
      "step 2\n",
      "loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(15.9313, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(13)\n",
      "torch.Size([26, 30])\n",
      "step 3\n",
      "loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(32.3432, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([32, 30])\n",
      "step 4\n",
      "loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(37.2153, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(32)\n",
      "torch.Size([25, 30])\n",
      "step 5\n",
      "loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(27.8402, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 6\n",
      "loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(31.7577, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 7\n",
      "loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(35.0495, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([26, 30])\n",
      "step 8\n",
      "loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(38.0354, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([25, 30])\n",
      "step 9\n",
      "loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(35.6890, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 10\n",
      "loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(32.3585, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 11\n",
      "loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.0264, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([28, 30])\n",
      "step 12\n",
      "loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(39.5568, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(27)\n",
      "torch.Size([26, 30])\n",
      "step 13\n",
      "loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(27.9819, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([29, 30])\n",
      "step 14\n",
      "loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(38.2670, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(29)\n",
      "torch.Size([30, 30])\n",
      "step 15\n",
      "loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(40.8256, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(29)\n",
      "torch.Size([25, 30])\n",
      "step 16\n",
      "loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.9914, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([33, 30])\n",
      "step 17\n",
      "loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(42.6493, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(33)\n",
      "torch.Size([29, 30])\n",
      "step 18\n",
      "loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(36.2124, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(28)\n",
      "torch.Size([29, 30])\n",
      "step 19\n",
      "loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(39.2030, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(29)\n",
      "torch.Size([25, 30])\n",
      "step 20\n",
      "loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(28.9830, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([28, 30])\n",
      "step 21\n",
      "loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(31.5781, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(28)\n",
      "torch.Size([25, 30])\n",
      "step 22\n",
      "loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.9456, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([26, 30])\n",
      "step 23\n",
      "loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(32.4321, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([28, 30])\n",
      "step 24\n",
      "loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(34.9850, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(28)\n",
      "Accurate_total tensor(631)\n",
      "Distance_total tensor(802.6943, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch_accuracy 0.9952681388012619\n",
      "Epoch_distance_avg 1.2660793899396372\n",
      "Saving..\n",
      "Epoch 26\n",
      "===========\n",
      "torch.Size([28, 30])\n",
      "step 1\n",
      "loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(36.3349, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(27)\n",
      "torch.Size([25, 30])\n",
      "step 2\n",
      "loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(28.9136, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([13, 30])\n",
      "step 3\n",
      "loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(18.3954, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(13)\n",
      "torch.Size([25, 30])\n",
      "step 4\n",
      "loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(29.8324, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 5\n",
      "loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.3550, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 6\n",
      "loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(33.5442, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([32, 30])\n",
      "step 7\n",
      "loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(39.0351, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(32)\n",
      "torch.Size([26, 30])\n",
      "step 8\n",
      "loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.9966, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([28, 30])\n",
      "step 9\n",
      "loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.1053, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(28)\n",
      "torch.Size([25, 30])\n",
      "step 10\n",
      "loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(33.1578, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([26, 30])\n",
      "step 11\n",
      "loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(32.1772, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([25, 30])\n",
      "step 12\n",
      "loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(27.3933, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([26, 30])\n",
      "step 13\n",
      "loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(29.4328, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([25, 30])\n",
      "step 14\n",
      "loss tensor(0.0060, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(33.8034, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 15\n",
      "loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.1781, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([29, 30])\n",
      "step 16\n",
      "loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(32.8865, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(29)\n",
      "torch.Size([25, 30])\n",
      "step 17\n",
      "loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(28.6052, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([33, 30])\n",
      "step 18\n",
      "loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(40.6528, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(33)\n",
      "torch.Size([29, 30])\n",
      "step 19\n",
      "loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(35.8292, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(29)\n",
      "torch.Size([26, 30])\n",
      "step 20\n",
      "loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(27.6686, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([29, 30])\n",
      "step 21\n",
      "loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(36.0792, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(27)\n",
      "torch.Size([30, 30])\n",
      "step 22\n",
      "loss tensor(0.0044, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(39.8736, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(30)\n",
      "torch.Size([26, 30])\n",
      "step 23\n",
      "loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(32.5971, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([28, 30])\n",
      "step 24\n",
      "loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(34.6651, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(28)\n",
      "Accurate_total tensor(631)\n",
      "Distance_total tensor(772.5123, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch_accuracy 0.9952681388012619\n",
      "Epoch_distance_avg 1.2184737204476213\n",
      "Saving..\n"
     ]
    }
   ],
   "source": [
    "encoder.train()\n",
    "# _hidden = encoder.initHidden()\n",
    "# Group batches/paddings by relative same-size\n",
    "\n",
    "accuracy_floor = .6\n",
    "distance_floor = 3.6\n",
    "\n",
    "for epoch in range(25,27):\n",
    "    \n",
    "    # modify for ordering -- benefit here with shuffling at ea epoch\n",
    "#     random.shuffle(train_ix)\n",
    "    random.shuffle(batches)\n",
    "    \n",
    "    print('Epoch',epoch)\n",
    "    print('===========')\n",
    "    step = 0\n",
    "    \n",
    "    distance = 0\n",
    "    accurate = 0\n",
    "    \n",
    "    # in future, mix up epochs \n",
    "    for i in range(len(batches)):\n",
    "        batch = batches[i]\n",
    "#     for i in range(19):\n",
    "        \n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "#         x,y,targets = get_minibatch(batchsize=BATCH_SIZE)\n",
    "        x,y,targets = get_minibatch(batchsize=len(batch))\n",
    "#         x = pad_minibatch(x)\n",
    "        \n",
    "#         y = torch.stack(y).double()\n",
    "        \n",
    "#         _hidden = encoder.initHidden(batch_size=BATCH_SIZE)\n",
    "        _hidden = encoder.initHidden(batch_size=len(batch))\n",
    "\n",
    "        yhat, _hidden = encoder(x, _hidden)\n",
    "\n",
    "        loss = loss_func(yhat,y)\n",
    "\n",
    "        loss.backward()\n",
    "#         loss.backward(retain_graph=True)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "        print('step',step)\n",
    "        print('loss',loss)\n",
    "\n",
    "        yhat_tensor = torch.tensor([out_to_score_proba(hat) for hat in yhat])\n",
    "        \n",
    "        _stepdistance = accuracy_distansum(yhat_tensor=yhat_tensor,y_tensor=targets)\n",
    "        distance += _stepdistance\n",
    "        \n",
    "        _stepaccurate = accuracy(yhat_tensor=yhat_tensor,y_tensor=targets)\n",
    "        accurate += _stepaccurate\n",
    "        \n",
    "        \n",
    "        print('step_accuracy_distance',_stepdistance)\n",
    "        print('step_accurate',_stepaccurate)\n",
    "    \n",
    "    print('Accurate_total',accurate)\n",
    "    print('Distance_total',distance)\n",
    "    \n",
    "    epoch_accuracy = accurate.item()/(len(train_ix))\n",
    "    # may be a bug in distance..\n",
    "    epoch_distance_avg = distance.item()/(len(train_ix))\n",
    "    \n",
    "    print('Epoch_accuracy',epoch_accuracy)\n",
    "    print('Epoch_distance_avg',epoch_distance_avg)\n",
    "    \n",
    "    if epoch_accuracy > accuracy_floor or epoch_distance_avg < distance_floor:\n",
    "        \n",
    "        print('Saving..')\n",
    "        torch.save(encoder.state_dict(),'encoder_accuracy_{:.3f}_avgdistance_{:3f}.pt'.format(epoch_accuracy, epoch_distance_avg))\n",
    "        \n",
    "        if epoch_accuracy > accuracy_floor: accuracy_floor = epoch_accuracy\n",
    "        if epoch_distance_avg > distance_floor: distance_floor = epoch_distance_avg\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load('./FinalProj/encoder_0.99accuracy_25epch.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (gru): GRU(371, 128, num_layers=2)\n",
       "  (fc): Linear(in_features=256, out_features=30, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trace,y_trace,targets_trace = get_embs(mem_ix=list(y_mem.keys()),tech_ix=list(y_tech.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_mem) + len(y_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hidden = encoder.initHidden(batch_size=len(y_mem)+len(y_tech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat, _hidden = encoder(x_trace, _hidden)\n",
    "# Rm non-pos\n",
    "estimates = torch.tensor([out_to_score_proba(hat) for hat in yhat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(targets_trace,estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.abs(targets_trace - estimates).sum())/len(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29, 29, 28, 28, 23, 28, 29, 23, 16, 29, 28, 16,  3, 28, 28, 29, 30, 29,\n",
       "        19, 28, 28, 29, 28, 29, 28, 23, 16, 24, 20, 29, 29, 13, 29, 28, 29, 28,\n",
       "        23, 30, 20, 28, 26, 28, 28, 28, 23, 28, 23, 28,  5, 19, 19, 29, 19, 20,\n",
       "        28, 19, 28, 28, 28, 20, 29, 30, 28, 28, 23, 19, 19, 28, 18, 17, 28, 28,\n",
       "        25, 29, 28, 20, 28, 20, 19, 16, 19, 28, 23, 28, 28, 20, 28, 19, 28, 28,\n",
       "        28, 23, 21, 29, 29, 28])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 30, 28, 30, 30, 30, 30, 29, 29, 29, 30, 30, 30, 29, 30, 30, 30, 30,\n",
       "        28, 29, 30, 28, 30, 29, 30, 26, 30, 29, 29, 30, 30, 29, 30, 28, 30, 30,\n",
       "        30, 30, 30, 30, 30, 30, 29, 30, 30, 29, 28, 30, 28, 30, 30, 30, 30, 29,\n",
       "        29, 29, 30, 30, 30, 29, 29, 30, 30, 30, 30, 30, 28, 29, 30, 28, 28, 30,\n",
       "        29, 30, 26, 30, 29, 29, 30, 30, 29, 30, 28, 30, 30, 30, 30, 30, 30, 30,\n",
       "        30, 29, 30, 30, 30, 29])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6/96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09375"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9/96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29, 29, 20, 28, 23, 29, 29, 23, 16, 29, 28, 16, 19, 28, 28, 29, 28, 19,\n",
       "        19, 25, 28, 29, 17, 29, 28, 23, 16, 24, 20, 29, 17, 29, 29, 28, 29, 28,\n",
       "        23, 30, 29, 28, 26, 28, 28, 19, 14, 28, 23, 16,  5, 25, 13, 29, 19, 19,\n",
       "        25, 19, 19,  3, 28, 20, 25, 30, 19, 29, 26, 19, 14, 28, 18, 17, 20, 28,\n",
       "        16, 29, 28, 15, 28, 20,  5, 16, 19, 20, 23, 24, 28, 20, 13, 19, 16, 28,\n",
       "        28, 23, 28, 19, 29, 28])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without pos tags - ix 13 'I have been able to recall - in detail -- memories from my first year.  This was confirmed by my Mother.' jumps from 3 to 19 \n",
    "# ...but pos is imp...tweak pos output?\n",
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.abs(targets_trace - estimates).sum())/len(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29, 30, 28, 28, 23, 29, 29, 23, 16, 29, 28, 16,  3, 28, 28, 29, 28, 19,\n",
       "        19, 28, 28, 29, 28, 29, 28, 23, 16, 24, 20, 29, 28, 13, 29, 28, 29, 28,\n",
       "        23, 30, 20, 28, 26, 28, 28, 28, 23, 28, 23, 16,  5, 19, 13, 29, 19, 11,\n",
       "        28, 19, 19,  3, 28, 28, 29, 30, 19, 28, 23, 19, 14, 28, 18, 17, 28, 28,\n",
       "        16, 29, 28, 15, 28, 20,  3, 16, 19, 28, 23, 24, 28, 20, 28, 19, 28, 28,\n",
       "        28, 23, 21, 29, 29, 28])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm on the computer all the time, writing or responding to social media. I've made friends. I keep in touch with friends. The bad thing about social media, though, is that it's an easy distraction. If the writing is going badly, it's all too easy to pop over to Facebook and pick a fight about politics.\""
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apc_datasheet.iloc[list(y_tech.keys())[3],3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 30, 28, 30, 30, 30, 30, 29, 29, 29, 30, 30, 30, 29, 30, 30, 30, 30,\n",
       "        28, 29, 30, 28, 30, 29, 30, 26, 30, 29, 29, 30, 30, 29, 30, 28, 30, 30,\n",
       "        30, 30, 30, 30, 30, 30, 29, 30, 30, 29, 28, 30, 28, 30, 30, 30, 30, 29,\n",
       "        29, 29, 30, 30, 30, 29, 29, 30, 30, 30, 30, 30, 28, 29, 30, 28, 28, 30,\n",
       "        29, 30, 26, 30, 29, 29, 30, 30, 29, 30, 28, 30, 30, 30, 30, 30, 30, 30,\n",
       "        30, 29, 30, 30, 30, 29])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.abs(targets_trace - estimates).sum())/len(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have been able to recall - in detail -- memories from my first year.  This was confirmed by my Mother.'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apc_datasheet.iloc[list(list(y_mem.keys())+list(y_tech.keys()))[13],2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't use social media, as I hear that it consumes a lot of time.  I only use email.  I don't have a Facebook account nor any other social media account.  I use my iphone regularly for email, calendar and notes fields, as well as the internet.    I don't feel the need to look into people's lives via social media, as I feel it could be depressing.  I'd rather email, speak or visit with friends or family.\""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apc_datasheet.iloc[list(list(y_mem.keys())+list(y_tech.keys()))[78],3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates[78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(targets_trace[:46],estimates[:46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13043478260869565"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6/46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.abs(targets_trace[:46] - estimates[:46]).sum())/46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(targets_trace[46:],estimates[46:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.abs(targets_trace[46:] - estimates[46:]).sum())/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03333333333333333"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-cceaff2385c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy_distansum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets_trace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-108-10c8f0800f2b>\u001b[0m in \u001b[0;36maccuracy_distansum\u001b[0;34m(yhat_tensor, y_tensor)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# not on y but on targets as mmse scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maccuracy_distansum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy_distansum(targets_trace,estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([158, 30])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder.eval()\n",
    "\n",
    "x_valid,y_valid,targets_valid = get_minibatch(ix=valid_ix)\n",
    "\n",
    "# x_valid = pad_minibatch(x_valid)\n",
    "\n",
    "# y_valid = torch.stack(y_valid).double()\n",
    "\n",
    "_hidden = encoder.initHidden(batch_size=len(valid_ix))\n",
    "\n",
    "yhat_valid, _hidden = encoder(x_valid, _hidden)\n",
    "estimates = torch.tensor([out_to_score_proba(hat) for hat in yhat_valid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 18, 29, 28, 26, 27, 23, 30, 19, 17, 20, 17, 25, 30, 23, 29, 27, 30,\n",
       "        19, 16, 30,  7, 28, 28, 28, 28, 25, 12, 30, 18, 30, 25, 15, 18, 14, 26,\n",
       "        27, 30, 20, 17, 30, 26, 19, 27, 17, 20, 20, 29, 27, 29, 30, 30, 30, 20,\n",
       "        22, 29, 19, 30, 30, 30, 30, 22, 17, 27, 27, 30, 30, 23, 30, 30, 19, 25,\n",
       "        30, 19, 30, 30, 30,  8, 30, 22, 30, 30, 28, 16, 19, 22, 24, 28, 16, 30,\n",
       "        30, 19, 17, 27, 19, 30, 30, 21, 28, 19, 29, 19, 12, 28, 20, 23, 13, 20,\n",
       "        23, 27, 23, 19, 20, 19, 19, 19, 27, 16, 10, 20, 17, 24, 24, 23, 16, 20,\n",
       "        18, 17, 19, 28, 29, 20, 27, 20, 23, 16, 20, 23, 27, 22, 19, 20, 20, 19,\n",
       "        19, 28, 28, 28, 26, 19, 24, 22, 20, 18, 23, 27, 25, 25])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 18, 29, 30, 26, 27, 23, 30, 19, 17, 20, 17, 25, 30, 23, 29, 27, 30,\n",
       "        19, 16, 30,  7, 28, 28, 28, 28, 25, 12, 30, 18, 30, 25, 15, 18, 14, 26,\n",
       "        27, 30, 20, 17, 30, 26, 19, 27, 17, 20, 20, 29, 27, 29, 30, 30, 30, 20,\n",
       "        22, 29, 19, 30, 30, 30, 30, 22, 17, 27, 27, 30, 30, 23, 30, 30, 19, 25,\n",
       "        30, 19, 30, 30, 30,  8, 30, 22, 30, 30, 28, 16, 19, 22, 24, 28, 16, 30,\n",
       "        30, 19, 17, 27, 19, 30, 30, 21, 28, 19, 29, 19, 12, 28, 20, 23, 13, 20,\n",
       "        23, 27, 23, 19, 20, 19, 19, 19, 27, 16, 10, 20, 17, 24, 24, 23, 16, 20,\n",
       "        18, 17, 19, 28, 29, 20, 27, 20, 23, 16, 20, 23, 27, 22, 19, 20, 20, 19,\n",
       "        19, 28, 28, 28, 26, 19, 24, 22, 20, 18, 23, 27, 25, 25])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for heatmap visualization\n",
    "# pd.Series(targets_valid).to_csv('targets_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(estimates).to_csv('estimates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.6159, dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is WRONG with this metric\n",
    "accuracy_distansum(targets_valid,estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09250569620253164"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14.6159/158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(157)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(targets_valid,estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9936708860759493"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "157/158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3]),)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(targets_valid != estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([634, 30])\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "\n",
    "x_train,y_train,targets_train = get_minibatch(ix=train_ix)\n",
    "\n",
    "# x_valid = pad_minibatch(x_valid)\n",
    "\n",
    "# y_valid = torch.stack(y_valid).double()\n",
    "\n",
    "_hidden = encoder.initHidden(batch_size=len(train_ix))\n",
    "yhat_train, _hidden = encoder(x_train, _hidden)\n",
    "estimates_train = torch.tensor([out_to_score_proba(hat) for hat in yhat_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 30, 12, 17, 30, 29, 20, 29, 15, 30, 29, 29, 29, 29, 30, 30, 28,  3,\n",
       "        28, 27, 30, 19, 27, 30, 10, 30, 30, 17, 23, 30, 30, 19, 30, 30, 30, 30,\n",
       "        30, 28, 30, 19, 29, 30, 30, 29, 30, 11, 27, 21, 19, 29, 30, 28, 30, 12,\n",
       "        30, 13, 22, 21, 29, 29, 17, 10, 23, 13, 18, 11, 28, 30, 18,  3, 29, 27,\n",
       "        30, 21, 20, 25, 30, 28, 30, 17, 24, 26, 20, 27, 30, 30, 19, 28, 24, 25,\n",
       "        20, 24, 30, 29, 16, 30, 30, 30, 30, 30, 19, 28, 15, 30, 30, 26, 30, 29,\n",
       "        30, 30, 30, 26, 30, 19, 26, 23, 18, 19, 13, 23, 12, 25, 30, 30, 10, 24,\n",
       "        30, 30, 28, 28, 15, 30, 30, 30, 24, 29, 28, 30, 28, 30, 23, 23, 20, 30,\n",
       "        29,  8, 20, 30, 19, 20, 25, 29, 20, 30, 30, 28, 30, 27, 29, 28, 22, 28,\n",
       "        25, 23, 29, 20, 23, 16, 18, 23, 19, 25, 30, 30, 17, 30, 30, 20, 10, 30,\n",
       "        18, 22, 24, 15, 29, 30, 19, 18, 28, 30, 30, 29, 30, 30, 30, 22, 29, 30,\n",
       "        29, 30, 26, 29, 15, 16, 30, 30, 13, 24, 30, 29, 23, 28, 13, 30, 28, 11,\n",
       "        29, 18, 18, 17, 27, 16, 12, 13, 29, 25, 30, 15, 30, 29, 10, 17, 11, 19,\n",
       "        10, 27, 22, 19, 28, 29, 28, 30, 19, 30, 26, 25, 13, 30, 22, 28, 29, 30,\n",
       "        13, 18, 23, 30, 24, 14, 30, 16, 30, 23, 30, 30, 15, 30, 30, 14, 18, 19,\n",
       "        10, 19, 24, 28, 19, 19, 30, 11, 28, 30, 29, 13, 17, 30, 12, 18, 18, 19,\n",
       "        30, 26, 29, 15, 17, 29, 25, 20, 28, 18, 30, 10, 26, 23, 21, 29, 19, 20,\n",
       "        20, 19, 20, 18, 29, 13, 30, 16, 28, 30, 29, 30, 17, 20, 24, 23, 23, 23,\n",
       "        30, 30, 30, 23, 30, 30, 27, 30, 19, 30, 18, 14, 29, 29, 27, 29, 29, 30,\n",
       "        19, 30, 17, 30, 28, 13, 14, 20, 30, 24, 15, 23, 30, 12, 30, 24, 29, 20,\n",
       "        30, 30, 30, 29, 14, 26, 29, 15, 19, 30, 20, 20, 30, 20, 28, 26, 29, 19,\n",
       "        22, 28, 29, 17, 29, 28, 12, 20, 29, 17,  5, 17, 30, 30, 29, 30, 29, 28,\n",
       "        30, 29, 25, 30, 24, 29, 21, 24, 30, 19, 19, 30, 25, 28, 17, 30,  1, 27,\n",
       "        12, 20, 30, 24, 27, 30, 27, 28, 28, 15, 19, 26, 20, 15, 26, 30, 28, 19,\n",
       "        30, 27, 30, 19, 30, 13, 28, 30, 28, 21, 20, 20, 30, 19, 13, 13, 21, 19,\n",
       "        29, 19, 19, 24, 17, 19, 13, 12, 26, 23, 18, 13, 15, 25, 20, 28, 21, 21,\n",
       "        16, 29, 24, 21, 17,  7, 29, 19, 16, 21, 19, 29, 20,  8, 26, 28, 18, 25,\n",
       "        11, 21, 23, 18, 17, 28, 19, 19, 28,  8, 23, 30, 19, 27, 17, 13, 28, 26,\n",
       "        23, 26, 28, 24, 19, 19, 15, 10, 20, 17, 27, 11, 30, 28, 12, 10, 22, 19,\n",
       "        28, 19, 22, 13, 11, 25, 11, 14, 17, 30, 21, 20, 24, 15, 18, 26, 12, 19,\n",
       "        23, 19, 11, 21, 26, 19, 25, 17,  7, 22, 30, 15, 13, 27, 23, 25, 25, 17,\n",
       "        19, 23, 27, 19, 23, 21, 13, 18, 18, 19, 19, 29, 25, 10, 28, 20, 29, 29,\n",
       "        30, 24, 27, 19, 20, 14, 19, 12, 27, 19, 23, 17, 18, 20, 13, 17, 14, 25,\n",
       "        14, 22, 20, 28, 29, 17, 18, 18, 29, 18, 23, 19, 19, 28, 25, 24, 19, 15,\n",
       "        13, 13, 20, 19, 20, 19, 26, 17, 28, 19, 19, 15, 19, 29, 18, 18, 10, 15,\n",
       "        30, 15, 25, 17])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_train_bugfix = torch.tensor([out_to_score_proba(hat) for hat in yhat_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 30, 12, 17, 30, 29, 20, 29, 15, 30, 29, 29, 29, 29, 30, 30, 28,  3,\n",
       "        28, 27, 30, 19, 27, 30, 10, 30, 30, 17, 23, 30, 30, 19, 30, 30, 30, 30,\n",
       "        30, 28, 30, 19, 29, 30, 30, 29, 30, 11, 27, 21, 19, 29, 30, 28, 30, 12,\n",
       "        30, 13, 22, 21, 29, 29, 17, 10, 23, 13, 18, 11, 28, 30, 18,  3, 29, 27,\n",
       "        30, 21, 20, 25, 30, 28, 30, 17, 24, 26, 20, 27, 30, 30, 19, 28, 24, 25,\n",
       "        20, 24, 30, 29, 16, 30, 30, 30, 30, 30, 19, 28, 15, 30, 30, 26, 30, 29,\n",
       "        30, 30, 30, 26, 30, 19, 26, 23, 18, 19, 13, 23, 12, 25, 30, 30, 10, 24,\n",
       "        30, 30, 28, 28, 15, 30, 30, 30, 24, 29, 28, 30, 28, 30, 23, 23, 20, 30,\n",
       "        29,  8, 20, 30, 19, 20, 25, 29, 20, 30, 30, 28, 30, 27, 29, 28, 22, 28,\n",
       "        25, 23, 29, 20, 23, 16, 18, 23, 19, 25, 30, 30, 17, 30, 30, 20, 10, 30,\n",
       "        18, 22, 24, 15, 29, 30, 19, 18, 28, 30, 30, 29, 30, 30, 30, 22, 29, 30,\n",
       "        29, 30, 26, 29, 15, 16, 30, 30, 13, 24, 30, 29, 23, 28, 13, 30, 28, 11,\n",
       "        29, 18, 18, 17, 27, 16, 12, 13, 29, 25, 30, 15, 30, 29, 10, 17, 11, 19,\n",
       "        10, 27, 22, 19, 28, 29, 28, 30, 19, 30, 26, 25, 13, 30, 22, 28, 29, 30,\n",
       "        13, 18, 23, 30, 24, 14, 30, 16, 30, 23, 30, 30, 15, 30, 30, 14, 18, 19,\n",
       "        10, 19, 24, 28, 19, 19, 30, 11, 28, 30, 29, 13, 17, 30, 12, 18, 18, 19,\n",
       "        30, 26, 29, 15, 17, 29, 25, 20, 28, 18, 30, 10, 26, 23, 21, 29, 19, 20,\n",
       "        20, 19, 20, 18, 29, 13, 30, 16, 28, 30, 29, 30, 17, 20, 24, 23, 23, 23,\n",
       "        30, 30, 30, 23, 30, 30, 27, 30, 19, 30, 18, 14, 29, 29, 27, 29, 29, 30,\n",
       "        19, 30, 17, 30, 28, 13, 14, 20, 30, 24, 15, 23, 30, 12, 30, 24, 29, 20,\n",
       "        30, 30, 30, 29, 14, 26, 29, 15, 19, 30, 20, 20, 30, 20, 28, 26, 29, 19,\n",
       "        22, 28, 29, 17, 29, 28, 12, 20, 29, 17,  5, 17, 30, 30, 29, 30, 29, 28,\n",
       "        30, 29, 25, 30, 24, 29, 21, 24, 30, 19, 19, 30, 25, 28, 17, 30,  1, 27,\n",
       "        12, 20, 30, 24, 27, 30, 27, 28, 28, 15, 19, 26, 20, 15, 26, 30, 28, 19,\n",
       "        30, 27, 30, 19, 30, 13, 28, 30, 28, 21, 20, 20, 30, 19, 13, 13, 21, 19,\n",
       "        29, 19, 19, 24, 17, 19, 13, 12, 26, 23, 18, 13, 15, 25, 20, 28, 21, 21,\n",
       "        16, 29, 24, 21, 17,  7, 29, 19, 16, 21, 19, 29, 20,  8, 26, 28, 18, 25,\n",
       "        11, 21, 23, 18, 17, 28, 19, 19, 28,  8, 23, 30, 19, 27, 17, 13, 28, 26,\n",
       "        23, 26, 28, 24, 19, 19, 15, 10, 20, 17, 27, 11, 30, 28, 12, 10, 22, 19,\n",
       "        28, 19, 19, 13, 11, 25, 11, 14, 17, 30, 21, 20, 24, 15, 18, 26, 12, 19,\n",
       "        23, 19, 11, 21, 26, 19, 25, 17,  7, 22, 30, 15, 13, 27, 23, 25, 25, 17,\n",
       "        19, 23, 27, 19, 23, 21, 13, 18, 18, 19, 19, 29, 25, 10, 28, 20, 29, 29,\n",
       "        30, 23, 27, 19, 20, 14, 19, 12, 27, 19, 23, 17, 18, 20, 13, 17, 14, 25,\n",
       "        14, 22, 20, 28, 29, 17, 18, 18, 29, 18, 23, 19, 19, 28, 25, 24, 19, 15,\n",
       "        13, 13, 20, 19, 20, 19, 26, 17, 28, 19, 19, 15, 19, 29, 18, 18, 10, 15,\n",
       "        30, 15, 25, 17])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates_train_bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(632)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(targets_train,estimates_train_bugfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9968454258675079"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "632/len(train_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.6159, dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_distansum(targets_train,estimates_train_bugfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02305347003154574"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14.6159/len(train_ix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python dl1010",
   "language": "python",
   "name": "dl1010"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
